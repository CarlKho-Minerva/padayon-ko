{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mic Recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... Press Enter to stop.\n",
      "Recording complete.\n",
      "Waiting for operation to complete...\n",
      "Transcript\n",
      "can you somehow find a way to make it. Streaming\n",
      " I really enjoy your day working a lot\n",
      " pretty tiring streaming\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "from google.cloud import speech\n",
    "import io\n",
    "import threading\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Initialize the Speech-to-Text client\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "# Function to record audio from the microphone\n",
    "def record_audio(temp_file_path):\n",
    "    # Setup parameters for the audio recording\n",
    "    chunk = 1024  # Record in chunks of 1024 samples\n",
    "    sample_format = pyaudio.paInt16  # 16 bits per sample\n",
    "    channels = 1\n",
    "    sample_rate = 24000  # Record at 24kHz\n",
    "    \n",
    "    p = pyaudio.PyAudio()  # Create an interface to PortAudio\n",
    "\n",
    "    print(\"Recording... Press Enter to stop.\")\n",
    "\n",
    "    # Open a new stream for recording\n",
    "    stream = p.open(format=sample_format,\n",
    "                    channels=channels,\n",
    "                    rate=sample_rate,\n",
    "                    frames_per_buffer=chunk,\n",
    "                    input=True)\n",
    "\n",
    "    frames = []  # Initialize array to store frames\n",
    "\n",
    "    # Function to read audio data in a loop\n",
    "    def read_audio():\n",
    "        while not stop_recording.is_set():\n",
    "            data = stream.read(chunk)\n",
    "            frames.append(data)\n",
    "\n",
    "    # Create a threading event to stop recording\n",
    "    stop_recording = threading.Event()\n",
    "\n",
    "    # Start the audio recording thread\n",
    "    recording_thread = threading.Thread(target=read_audio)\n",
    "    recording_thread.start()\n",
    "\n",
    "    # Wait for user input to stop recording\n",
    "    input()\n",
    "    stop_recording.set()\n",
    "    recording_thread.join()\n",
    "\n",
    "    # Stop and close the stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    print(\"Recording complete.\")\n",
    "\n",
    "    # Save the recorded data as a WAV file\n",
    "    with wave.open(temp_file_path, 'wb') as wf:\n",
    "        wf.setnchannels(channels)\n",
    "        wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "# Function to transcribe audio using Google Cloud Speech-to-Text\n",
    "def transcribe_audio(temp_file_path):\n",
    "    # Read the audio file\n",
    "    with io.open(temp_file_path, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "    \n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=24000,\n",
    "        language_code=\"ceb-PH\",\n",
    "        model=\"default\",\n",
    "        audio_channel_count=1,\n",
    "        enable_word_confidence=True,\n",
    "        enable_word_time_offsets=True,\n",
    "    )\n",
    "\n",
    "    # Detects speech in the audio file\n",
    "    operation = client.long_running_recognize(config=config, audio=audio)\n",
    "\n",
    "    print(\"Waiting for operation to complete...\")\n",
    "    response = operation.result(timeout=90)\n",
    "\n",
    "    print(\"Transcript\")\n",
    "    for result in response.results:\n",
    "        print(result.alternatives[0].transcript)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n",
    "        temp_file_path = temp_file.name\n",
    "\n",
    "    try:\n",
    "        record_audio(temp_file_path)\n",
    "        transcribe_audio(temp_file_path)\n",
    "    finally:\n",
    "        os.remove(temp_file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech\n",
    "\n",
    "def synthesize_text(text):\n",
    "    \"\"\"Synthesizes speech from the input string of text.\"\"\"\n",
    "\n",
    "\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "\n",
    "    # Note: the voice can also be specified by name.\n",
    "    # Names of voices can be retrieved with client.list_voices().\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"en-US\",\n",
    "        name=\"en-US-Standard-C\",\n",
    "        ssml_gender=texttospeech.SsmlVoiceGender.FEMALE,\n",
    "    )\n",
    "\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "\n",
    "    response = client.synthesize_speech(\n",
    "        request={\"input\": input_text, \"voice\": voice, \"audio_config\": audio_config}\n",
    "    )\n",
    "\n",
    "    # The response's audio_content is binary.\n",
    "    with open(\"output.mp3\", \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "        print('Audio content written to file \"output.mp3\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio content written to file \"output.mp3\"\n"
     ]
    }
   ],
   "source": [
    "synthesize_text(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 1 - Terminal Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... Press Enter to stop.\n",
      "Recording complete.\n",
      "Transcribing audio...\n",
      "User said: hello I am testing this out\n",
      "AI response: Hello there! I'm glad you're here to test me out. I'm eager to engage in a thought-provoking debate or lend my support in defending an idea. Please feel free to present your stance, and I'll do my best to stimulate a lively and enriching conversation. Let's get started!\n",
      "Audio response saved as \"response.mp3\"\n",
      "Recording... Press Enter to stop.\n",
      "Recording complete.\n",
      "Transcribing audio...\n",
      "User said: so I think abortion should be illegal\n",
      "AI response: Abortion is a complex and highly personal issue. There are many different beliefs about when life begins, and what rights a fetus has. It is important to be respectful of all opinions on this topic, even if you disagree with them.\n",
      "\n",
      "One of the main reasons why people believe abortion should be illegal is because they believe that life begins at conception. They argue that a fetus is a human being with the same rights as any other human, and that abortion is therefore murder.\n",
      "\n",
      "Others believe that abortion should be legal because they believe that a woman has the right to control her own body. They argue that the government should not interfere in a woman's decision to have an abortion.\n",
      "\n",
      "There is no easy answer to the question of whether or not abortion should be legal. It is a complex issue with many different perspectives. It is important to be respectful of all opinions on this topic, even if you disagree with them.\n",
      "Audio response saved as \"response.mp3\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyaudio\n",
    "import wave\n",
    "import io\n",
    "import threading\n",
    "import tempfile\n",
    "from google.cloud import speech\n",
    "from google.cloud import texttospeech\n",
    "import google.generativeai as genai\n",
    "import random\n",
    "\n",
    "# Initialize the Speech-to-Text client\n",
    "speech_client = speech.SpeechClient()\n",
    "\n",
    "# Initialize the Text-to-Speech client\n",
    "tts_client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "# Initialize Gemini API\n",
    "genai.configure(api_key=os.environ['GEMINI_API_KEY'])\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "def record_audio(temp_file_path):\n",
    "    # Setup parameters for the audio recording\n",
    "    chunk = 1024\n",
    "    sample_format = pyaudio.paInt16\n",
    "    channels = 1\n",
    "    sample_rate = 24000\n",
    "    \n",
    "    p = pyaudio.PyAudio()\n",
    "    print(\"Recording... Press Enter to stop.\")\n",
    "    \n",
    "    stream = p.open(format=sample_format,\n",
    "                    channels=channels,\n",
    "                    rate=sample_rate,\n",
    "                    frames_per_buffer=chunk,\n",
    "                    input=True)\n",
    "    \n",
    "    frames = []\n",
    "    stop_recording = threading.Event()\n",
    "    \n",
    "    def read_audio():\n",
    "        while not stop_recording.is_set():\n",
    "            data = stream.read(chunk)\n",
    "            frames.append(data)\n",
    "    \n",
    "    recording_thread = threading.Thread(target=read_audio)\n",
    "    recording_thread.start()\n",
    "    \n",
    "    input()\n",
    "    stop_recording.set()\n",
    "    recording_thread.join()\n",
    "    \n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    \n",
    "    print(\"Recording complete.\")\n",
    "    \n",
    "    with wave.open(temp_file_path, 'wb') as wf:\n",
    "        wf.setnchannels(channels)\n",
    "        wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "def transcribe_audio(temp_file_path):\n",
    "    with io.open(temp_file_path, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "    \n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=24000,\n",
    "        language_code=\"en-US\",\n",
    "        model=\"default\",\n",
    "        audio_channel_count=1,\n",
    "        enable_word_confidence=True,\n",
    "        enable_word_time_offsets=True,\n",
    "    )\n",
    "    \n",
    "    operation = speech_client.long_running_recognize(config=config, audio=audio)\n",
    "    print(\"Transcribing audio...\")\n",
    "    response = operation.result(timeout=90)\n",
    "    \n",
    "    transcript = \"\"\n",
    "    for result in response.results:\n",
    "        transcript += result.alternatives[0].transcript + \" \"\n",
    "    \n",
    "    return transcript.strip()\n",
    "\n",
    "def synthesize_speech(text):\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"en-US\",\n",
    "        name=\"en-US-Standard-C\",\n",
    "        ssml_gender=texttospeech.SsmlVoiceGender.FEMALE,\n",
    "    )\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "    \n",
    "    response = tts_client.synthesize_speech(\n",
    "        input=input_text, voice=voice, audio_config=audio_config\n",
    "    )\n",
    "    \n",
    "    with open(\"response.mp3\", \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "    print('Audio response saved as \"response.mp3\"')\n",
    "\n",
    "def generate_ai_response(user_input, mode):\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI communication assistant. Based on the user's input, your task is to {mode}. \n",
    "    Respond in a way that encourages further conversation and helps the user improve their communication skills.\n",
    "\n",
    "    User input: {user_input}\n",
    "\n",
    "    Your response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n",
    "            temp_file_path = temp_file.name\n",
    "        \n",
    "        try:\n",
    "            record_audio(temp_file_path)\n",
    "            user_input = transcribe_audio(temp_file_path)\n",
    "            print(f\"User said: {user_input}\")\n",
    "            \n",
    "            mode = input(\"Choose mode (1: Debate, 2: Explain, 3: Storytelling, 4: Q&A): \")\n",
    "            modes = {\n",
    "                \"1\": \"debate or defend the idea presented\",\n",
    "                \"2\": \"explain the concept using metaphors and without circular reasoning\",\n",
    "                \"3\": \"create a narrative or story based on the input\",\n",
    "                \"4\": \"formulate relevant questions or provide answers based on the input\"\n",
    "            }\n",
    "            \n",
    "            ai_response = generate_ai_response(user_input, modes.get(mode, \"have a general conversation about\"))\n",
    "            print(f\"AI response: {ai_response}\")\n",
    "            \n",
    "            synthesize_speech(ai_response)\n",
    "            \n",
    "            continue_chat = input(\"Continue chatting? (y/n): \").lower()\n",
    "            if continue_chat != 'y':\n",
    "                break\n",
    "        \n",
    "        finally:\n",
    "            os.remove(temp_file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 2 - With html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from google.cloud import speech, texttospeech\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import tempfile\n",
    "import base64\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Initialize clients\n",
    "speech_client = speech.SpeechClient()\n",
    "tts_client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "# Initialize Gemini API\n",
    "genai.configure(api_key=\"YOUR_GEMINI_API_KEY\")\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "def transcribe_audio(audio_content):\n",
    "    audio = speech.RecognitionAudio(content=audio_content)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=24000,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "    response = speech_client.recognize(config=config, audio=audio)\n",
    "    return response.results[0].alternatives[0].transcript\n",
    "\n",
    "def synthesize_text(text):\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"en-US\",\n",
    "        name=\"en-US-Standard-C\",\n",
    "        ssml_gender=texttospeech.SsmlVoiceGender.FEMALE,\n",
    "    )\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "    response = tts_client.synthesize_speech(\n",
    "        input=input_text, voice=voice, audio_config=audio_config\n",
    "    )\n",
    "    return base64.b64encode(response.audio_content).decode('utf-8')\n",
    "\n",
    "def gemini_process(user_input, mode):\n",
    "    prompt = f\"\"\"\n",
    "    Respond to the user's input in a conversational manner, keeping the response around 5 seconds long when spoken.\n",
    "    Focus on the following mode: {mode}\n",
    "\n",
    "    1. Debate/Defend: Provide a counterargument or supporting argument.\n",
    "    2. Narrate/Storytell: Create a brief narrative or story element related to the input.\n",
    "    3. Explain: Use a metaphor or analogy to explain the concept.\n",
    "    4. Question Formation/Answering: Generate a relevant question or provide a concise answer.\n",
    "\n",
    "    User input: {user_input}\n",
    "    \"\"\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "@app.route('/process_audio', methods=['POST'])\n",
    "def process_audio():\n",
    "    audio_data = request.files['audio'].read()\n",
    "    mode = request.form['mode']\n",
    "    \n",
    "    user_input = transcribe_audio(audio_data)\n",
    "    ai_response = gemini_process(user_input, mode)\n",
    "    audio_response = synthesize_text(ai_response)\n",
    "    \n",
    "    return jsonify({\n",
    "        'user_input': user_input,\n",
    "        'ai_response': ai_response,\n",
    "        'audio_response': audio_response\n",
    "    })\n",
    "\n",
    "@app.route('/get_prompt', methods=['GET'])\n",
    "def get_prompt():\n",
    "    prompts = [\n",
    "        \"Discuss a recent technological advancement that excites you.\",\n",
    "        \"Share your thoughts on the future of remote work.\",\n",
    "        \"What's a book or movie that has greatly influenced your thinking?\",\n",
    "        \"Describe an interesting cultural tradition from your background.\",\n",
    "        \"If you could solve one global problem, what would it be and why?\"\n",
    "    ]\n",
    "    return jsonify({'prompt': random.choice(prompts)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\63916\\Downloads\\test-pk\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "  <head>\n",
    "    <meta charset=\"UTF-8\" />\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "    <title>AI Communication Practice</title>\n",
    "    <style>\n",
    "      body {\n",
    "        font-family: Arial, sans-serif;\n",
    "        line-height: 1.6;\n",
    "        color: #333;\n",
    "        max-width: 800px;\n",
    "        margin: 0 auto;\n",
    "        padding: 20px;\n",
    "      }\n",
    "      h1 {\n",
    "        text-align: center;\n",
    "        color: #2c3e50;\n",
    "      }\n",
    "      #prompt-container {\n",
    "        background-color: #ecf0f1;\n",
    "        padding: 20px;\n",
    "        border-radius: 5px;\n",
    "        margin-bottom: 20px;\n",
    "      }\n",
    "      #randomize-btn {\n",
    "        display: block;\n",
    "        margin: 10px auto;\n",
    "        padding: 10px 20px;\n",
    "        background-color: #3498db;\n",
    "        color: white;\n",
    "        border: none;\n",
    "        border-radius: 5px;\n",
    "        cursor: pointer;\n",
    "      }\n",
    "      .tab {\n",
    "        overflow: hidden;\n",
    "        border: 1px solid #ccc;\n",
    "        background-color: #f1f1f1;\n",
    "      }\n",
    "      .tab button {\n",
    "        background-color: inherit;\n",
    "        float: left;\n",
    "        border: none;\n",
    "        outline: none;\n",
    "        cursor: pointer;\n",
    "        padding: 14px 16px;\n",
    "        transition: 0.3s;\n",
    "      }\n",
    "      .tab button:hover {\n",
    "        background-color: #ddd;\n",
    "      }\n",
    "      .tab button.active {\n",
    "        background-color: #ccc;\n",
    "      }\n",
    "      .tabcontent {\n",
    "        display: none;\n",
    "        padding: 6px 12px;\n",
    "        border: 1px solid #ccc;\n",
    "        border-top: none;\n",
    "      }\n",
    "      #record-btn {\n",
    "        display: block;\n",
    "        margin: 20px auto;\n",
    "        padding: 15px 30px;\n",
    "        background-color: #e74c3c;\n",
    "        color: white;\n",
    "        border: none;\n",
    "        border-radius: 50%;\n",
    "        font-size: 18px;\n",
    "        cursor: pointer;\n",
    "      }\n",
    "      #output-container {\n",
    "        margin-top: 20px;\n",
    "        background-color: #ecf0f1;\n",
    "        padding: 20px;\n",
    "        border-radius: 5px;\n",
    "      }\n",
    "    </style>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>AI Communication Practice</h1>\n",
    "\n",
    "    <div id=\"prompt-container\">\n",
    "      <h2>Water Cooler Prompt:</h2>\n",
    "      <p id=\"prompt-text\"></p>\n",
    "      <button id=\"randomize-btn\">Randomize Prompt</button>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"tab\">\n",
    "      <button class=\"tablinks\" onclick=\"openMode(event, 'Debate')\">\n",
    "        Debate/Defend\n",
    "      </button>\n",
    "      <button class=\"tablinks\" onclick=\"openMode(event, 'Narrate')\">\n",
    "        Narrate/Storytell\n",
    "      </button>\n",
    "      <button class=\"tablinks\" onclick=\"openMode(event, 'Explain')\">\n",
    "        Explain\n",
    "      </button>\n",
    "      <button class=\"tablinks\" onclick=\"openMode(event, 'Question')\">\n",
    "        Question Formation/Answering\n",
    "      </button>\n",
    "    </div>\n",
    "\n",
    "    <div id=\"Debate\" class=\"tabcontent\">\n",
    "      <h3>Debate/Defend Guidelines</h3>\n",
    "      <ul>\n",
    "        <li>Clearly state your position on the topic</li>\n",
    "        <li>Provide evidence or logical reasoning to support your argument</li>\n",
    "        <li>Anticipate and address potential counterarguments</li>\n",
    "        <li>Use persuasive language and tone</li>\n",
    "        <li>Conclude with a strong restatement of your position</li>\n",
    "      </ul>\n",
    "    </div>\n",
    "\n",
    "    <div id=\"Narrate\" class=\"tabcontent\">\n",
    "      <h3>Narrate/Storytell Guidelines</h3>\n",
    "      <ul>\n",
    "        <li>Set the scene with vivid descriptions</li>\n",
    "        <li>Introduce interesting characters or elements</li>\n",
    "        <li>Create a clear plot or sequence of events</li>\n",
    "        <li>Use dialogue to bring the story to life</li>\n",
    "        <li>End with a satisfying conclusion or cliffhanger</li>\n",
    "      </ul>\n",
    "    </div>\n",
    "\n",
    "    <div id=\"Explain\" class=\"tabcontent\">\n",
    "      <h3>Explain Guidelines</h3>\n",
    "      <ul>\n",
    "        <li>Start with a clear, concise definition of the concept</li>\n",
    "        <li>Use relatable metaphors or analogies</li>\n",
    "        <li>Break down complex ideas into simpler components</li>\n",
    "        <li>Provide concrete examples to illustrate your points</li>\n",
    "        <li>Summarize key takeaways at the end</li>\n",
    "      </ul>\n",
    "    </div>\n",
    "\n",
    "    <div id=\"Question\" class=\"tabcontent\">\n",
    "      <h3>Question Formation/Answering Guidelines</h3>\n",
    "      <ul>\n",
    "        <li>For questions: Start with who, what, when, where, why, or how</li>\n",
    "        <li>Ensure questions are clear, concise, and focused</li>\n",
    "        <li>For answers: Directly address the main point of the question</li>\n",
    "        <li>Provide context or background information when necessary</li>\n",
    "        <li>\n",
    "          If unsure, acknowledge limitations and suggest where to find more\n",
    "          information\n",
    "        </li>\n",
    "      </ul>\n",
    "    </div>\n",
    "\n",
    "    <button id=\"record-btn\">Record</button>\n",
    "\n",
    "    <div id=\"output-container\">\n",
    "      <h3>Your Input:</h3>\n",
    "      <p id=\"user-input\"></p>\n",
    "      <h3>AI Response:</h3>\n",
    "      <p id=\"ai-response\"></p>\n",
    "      <audio id=\"audio-response\" controls></audio>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "      let currentMode = \"Debate\";\n",
    "      let isRecording = false;\n",
    "      let mediaRecorder;\n",
    "      let audioChunks = [];\n",
    "\n",
    "      function openMode(evt, modeName) {\n",
    "        var i, tabcontent, tablinks;\n",
    "        tabcontent = document.getElementsByClassName(\"tabcontent\");\n",
    "        for (i = 0; i < tabcontent.length; i++) {\n",
    "          tabcontent[i].style.display = \"none\";\n",
    "        }\n",
    "        tablinks = document.getElementsByClassName(\"tablinks\");\n",
    "        for (i = 0; i < tablinks.length; i++) {\n",
    "          tablinks[i].className = tablinks[i].className.replace(\" active\", \"\");\n",
    "        }\n",
    "        document.getElementById(modeName).style.display = \"block\";\n",
    "        evt.currentTarget.className += \" active\";\n",
    "        currentMode = modeName;\n",
    "      }\n",
    "\n",
    "      document\n",
    "        .getElementById(\"randomize-btn\")\n",
    "        .addEventListener(\"click\", getNewPrompt);\n",
    "      document\n",
    "        .getElementById(\"record-btn\")\n",
    "        .addEventListener(\"click\", toggleRecording);\n",
    "\n",
    "      function getNewPrompt() {\n",
    "        fetch(\"/get_prompt\")\n",
    "          .then((response) => response.json())\n",
    "          .then((data) => {\n",
    "            document.getElementById(\"prompt-text\").textContent = data.prompt;\n",
    "          });\n",
    "      }\n",
    "\n",
    "      function toggleRecording() {\n",
    "        if (!isRecording) {\n",
    "          startRecording();\n",
    "        } else {\n",
    "          stopRecording();\n",
    "        }\n",
    "      }\n",
    "\n",
    "      function startRecording() {\n",
    "        navigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {\n",
    "          mediaRecorder = new MediaRecorder(stream, {\n",
    "            mimeType: \"audio/webm;codecs=opus\",\n",
    "            audioBitsPerSecond: 128000,\n",
    "          });\n",
    "          mediaRecorder.start();\n",
    "\n",
    "          mediaRecorder.addEventListener(\"dataavailable\", (event) => {\n",
    "            audioChunks.push(event.data);\n",
    "          });\n",
    "\n",
    "          mediaRecorder.addEventListener(\"stop\", () => {\n",
    "            const audioBlob = new Blob(audioChunks, {\n",
    "              type: \"audio/webm;codecs=opus\",\n",
    "            });\n",
    "            sendAudioToServer(audioBlob);\n",
    "          });\n",
    "\n",
    "          isRecording = true;\n",
    "          document.getElementById(\"record-btn\").textContent = \"Stop\";\n",
    "        });\n",
    "      }\n",
    "\n",
    "      function stopRecording() {\n",
    "        mediaRecorder.stop();\n",
    "        isRecording = false;\n",
    "        document.getElementById(\"record-btn\").textContent = \"Record\";\n",
    "      }\n",
    "\n",
    "      function sendAudioToServer(audioBlob) {\n",
    "        const formData = new FormData();\n",
    "        formData.append(\"audio\", audioBlob, \"recording.webm\"); // Changed to .webm\n",
    "        formData.append(\"mode\", currentMode);\n",
    "\n",
    "        fetch(\"/process_audio\", {\n",
    "          method: \"POST\",\n",
    "          body: formData,\n",
    "        })\n",
    "          .then((response) => {\n",
    "            if (!response.ok) {\n",
    "              return response.json().then((err) => {\n",
    "                throw err;\n",
    "              });\n",
    "            }\n",
    "            return response.json();\n",
    "          })\n",
    "          .then((data) => {\n",
    "            document.getElementById(\"user-input\").textContent = data.user_input;\n",
    "            document.getElementById(\"ai-response\").textContent =\n",
    "              data.ai_response;\n",
    "\n",
    "            const audio = document.getElementById(\"audio-response\");\n",
    "            audio.src = \"data:audio/mp3;base64,\" + data.audio_response;\n",
    "            audio.play();\n",
    "          })\n",
    "          .catch((error) => {\n",
    "            console.error(\"Error:\", error);\n",
    "            document.getElementById(\"user-input\").textContent =\n",
    "              \"Error: \" + (error.error || \"Failed to process audio\");\n",
    "            document.getElementById(\"ai-response\").textContent = \"\";\n",
    "            document.getElementById(\"audio-response\").src = \"\";\n",
    "          });\n",
    "      }\n",
    "\n",
    "      // Initialize with a prompt and open the Debate tab\n",
    "      getNewPrompt();\n",
    "      document.getElementsByClassName(\"tablinks\")[0].click();\n",
    "    </script>\n",
    "  </body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration # 3 -- Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
