{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the SAT Math Preparation Chatbot!\n",
      "Let's start by understanding your interests.\n",
      "[DEBUG] Entering chatbot\n",
      "[DEBUG] Entering preprocess_input\n",
      "[DEBUG] Preprocessed input: Please provide the input that you would like me to preprocess. \n",
      "\n",
      "[DEBUG] Entering classify_input\n",
      "[DEBUG] Classified as: socratic_dialogue\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Content' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 297\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThank you for using the SAT Math Preparation Chatbot. Goodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 297\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchatbot\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[59], line 257\u001b[0m, in \u001b[0;36mchatbot\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m    255\u001b[0m     chat_session\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts\u001b[39m\u001b[38;5;124m\"\u001b[39m: [response]})\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m category \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msocratic_dialogue\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 257\u001b[0m     last_question_analysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchat_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQuestion Analysis:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m last_question_analysis:\n\u001b[0;32m    259\u001b[0m         question_analysis \u001b[38;5;241m=\u001b[39m last_question_analysis[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion Analysis:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "Cell \u001b[1;32mIn[59], line 257\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    255\u001b[0m     chat_session\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts\u001b[39m\u001b[38;5;124m\"\u001b[39m: [response]})\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m category \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msocratic_dialogue\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 257\u001b[0m     last_question_analysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m((msg \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(chat_session\u001b[38;5;241m.\u001b[39mhistory) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion Analysis:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmsg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m last_question_analysis:\n\u001b[0;32m    259\u001b[0m         question_analysis \u001b[38;5;241m=\u001b[39m last_question_analysis[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion Analysis:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Content' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Global debug variable\n",
    "debug = True\n",
    "\n",
    "def debug_print(message):\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] {message}\")\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-pro\",\n",
    "    generation_config=generation_config,\n",
    "    tools=['code_execution'],\n",
    ")\n",
    "\n",
    "chat_session = model.start_chat(history=[])\n",
    "\n",
    "def preprocess_input(user_input):\n",
    "    debug_print(\"Entering preprocess_input\")\n",
    "    prompt = \"\"\"\n",
    "    # High-level instructions:\n",
    "    1. Remove filler words and clean up the input.\n",
    "    2. Maintain the original idea without deviating from the main point.\n",
    "    3. If it's a math question, make it clear for future processing without rephrasing.\n",
    "    4. If it's not a math question, clarify the user's intent.\n",
    "\n",
    "    # Example input/output:\n",
    "    Input: \"Um, can you like help me solve this equation thing? It's 2x + 5 = 13 or something.\"\n",
    "    Output: \"Help solve the equation: 2x + 5 = 13\"\n",
    "\n",
    "    # Now, preprocess the following input:\n",
    "    {user_input}\n",
    "    \"\"\"\n",
    "    response = chat_session.send_message(prompt.format(user_input=user_input))\n",
    "    cleaned_input = response.text\n",
    "    debug_print(f\"Preprocessed input: {cleaned_input}\")\n",
    "    return cleaned_input\n",
    "\n",
    "def classify_input(cleaned_input):\n",
    "    debug_print(\"Entering classify_input\")\n",
    "    prompt = \"\"\"\n",
    "    # High-level instructions:\n",
    "    1. Analyze the given input and classify it into one of the following categories:\n",
    "       - interest_profiling\n",
    "       - question_analysis\n",
    "       - socratic_dialogue\n",
    "       - answer_verification\n",
    "       - learning_reinforcement\n",
    "    2. Base your classification on the overall intent of the input, not just keywords.\n",
    "    3. Return only the category name, nothing else.\n",
    "\n",
    "    # Example input/output:\n",
    "    Input: \"What are some math concepts used in basketball?\"\n",
    "    Output: interest_profiling\n",
    "\n",
    "    Input: \"Can you help me understand this equation: 2x + 5 = 13?\"\n",
    "    Output: question_analysis\n",
    "\n",
    "    # Now, classify the following input:\n",
    "    {cleaned_input}\n",
    "    \"\"\"\n",
    "    response = chat_session.send_message(prompt.format(cleaned_input=cleaned_input))\n",
    "    category = response.text.strip().lower()\n",
    "    debug_print(f\"Classified as: {category}\")\n",
    "    return category\n",
    "\n",
    "def interest_profiling_agent(user_input):\n",
    "    debug_print(\"Entering interest_profiling_agent\")\n",
    "    prompt = \"\"\"\n",
    "    # High-level instructions:\n",
    "    1. Engage the user in a friendly conversation to determine their interests.\n",
    "    2. Focus on how their interests relate to mathematics.\n",
    "    3. Prepare a brief interest profile that can be used in future math discussions.\n",
    "    4. Return the interest profile in a concise format.\n",
    "\n",
    "    # Example input/output:\n",
    "    Input: \"I love playing basketball and video games.\"\n",
    "    Output: \"Interest Profile: Sports (Basketball), Gaming. Math Connections: Geometry, Statistics, Probability.\"\n",
    "\n",
    "    # Now, create an interest profile based on the user's input:\n",
    "    {user_input}\n",
    "    \"\"\"\n",
    "    response = chat_session.send_message(prompt.format(user_input=user_input))\n",
    "    return response.text\n",
    "\n",
    "def question_analysis_agent(user_input):\n",
    "    debug_print(\"Entering question_analysis_agent\")\n",
    "    prompt = \"\"\"\n",
    "    # High-level instructions:\n",
    "    1. Analyze the given math question or problem.\n",
    "    2. Identify the key mathematical concepts involved.\n",
    "    3. Suggest a step-by-step approach to solving the problem.\n",
    "    4. Do not solve the problem, but prepare it for Socratic dialogue.\n",
    "\n",
    "    # Example input/output:\n",
    "    Input: \"Solve the equation: 2x + 5 = 13\"\n",
    "    Output: \"\n",
    "    Problem: Solve the equation: 2x + 5 = 13\n",
    "    Key Concepts: Linear equations, Algebraic manipulation\n",
    "    Suggested Approach:\n",
    "    1. Identify the variable and constants\n",
    "    2. Isolate the variable term on one side\n",
    "    3. Use inverse operations to solve for x\n",
    "    4. Verify the solution\n",
    "    \"\n",
    "\n",
    "    # Now, analyze the following math problem:\n",
    "    {user_input}\n",
    "    \"\"\"\n",
    "    response = chat_session.send_message(prompt.format(user_input=user_input))\n",
    "    return response.text\n",
    "\n",
    "def socratic_dialogue_agent(user_input, question_analysis):\n",
    "    debug_print(\"Entering socratic_dialogue_agent\")\n",
    "    prompt = \"\"\"\n",
    "    # High-level instructions:\n",
    "    1. Use the question analysis provided to guide a Socratic dialogue.\n",
    "    2. Ask open-ended questions that encourage critical thinking.\n",
    "    3. Provide hints and scaffolding when necessary, but avoid giving direct answers.\n",
    "    4. Use the user's interests (if available) to make connections to the math concepts.\n",
    "    5. Focus on one step of the problem-solving process at a time.\n",
    "\n",
    "    # Example input/output:\n",
    "    Input: \n",
    "    User: \"I'm not sure how to start solving 2x + 5 = 13\"\n",
    "    Question Analysis: \"\n",
    "    Problem: Solve the equation: 2x + 5 = 13\n",
    "    Key Concepts: Linear equations, Algebraic manipulation\n",
    "    Suggested Approach:\n",
    "    1. Identify the variable and constants\n",
    "    2. Isolate the variable term on one side\n",
    "    3. Use inverse operations to solve for x\n",
    "    4. Verify the solution\n",
    "    \"\n",
    "    Interest Profile: Sports (Basketball), Gaming\n",
    "\n",
    "    Output: \"Let's approach this step-by-step, relating it to basketball where we can. In basketball, you often need to isolate a player to create a scoring opportunity. Similarly, in this equation, we need to isolate our 'player' - the variable x. What do you think is the first step to isolate x in this equation?\"\n",
    "\n",
    "    # Now, engage in a Socratic dialogue based on the following input:\n",
    "    User Input: {user_input}\n",
    "    Question Analysis: {question_analysis}\n",
    "    \"\"\"\n",
    "    response = chat_session.send_message(prompt.format(user_input=user_input, question_analysis=question_analysis))\n",
    "    return response.text\n",
    "\n",
    "def answer_verification_agent(user_input, question_analysis):\n",
    "    debug_print(\"Entering answer_verification_agent\")\n",
    "    prompt = \"\"\"\n",
    "    # High-level instructions:\n",
    "    1. Analyze the user's proposed solution or answer.\n",
    "    2. Use Python code to verify the mathematical correctness of the answer.\n",
    "    3. Provide feedback on the correctness of the solution.\n",
    "    4. If the answer is incorrect, offer constructive feedback and hints for improvement.\n",
    "    5. If the answer is correct, congratulate the user and explain why their approach worked.\n",
    "\n",
    "    # Example input/output:\n",
    "    Input: \n",
    "    User: \"I think the answer to 2x + 5 = 13 is x = 4\"\n",
    "    Question Analysis: \"\n",
    "    Problem: Solve the equation: 2x + 5 = 13\n",
    "    Key Concepts: Linear equations, Algebraic manipulation\n",
    "    \"\n",
    "\n",
    "    Output: \n",
    "    ```python\n",
    "    # Verifying the solution\n",
    "    x = 4\n",
    "    left_side = 2 * x + 5\n",
    "    right_side = 13\n",
    "    is_correct = left_side == right_side\n",
    "    print(f\"Left side: {left_side}\")\n",
    "    print(f\"Right side: {right_side}\")\n",
    "    print(f\"Is the solution correct? {is_correct}\")\n",
    "    ```\n",
    "\n",
    "    The Python code shows that:\n",
    "    Left side: 13\n",
    "    Right side: 13\n",
    "    Is the solution correct? True\n",
    "\n",
    "    Great job! Your solution x = 4 is correct. When we plug x = 4 into the equation 2x + 5 = 13, we get 2(4) + 5 = 13, which simplifies to 8 + 5 = 13, and 13 = 13 is true. You've successfully solved the equation!\n",
    "\n",
    "    # Now, verify the following answer:\n",
    "    User Input: {user_input}\n",
    "    Question Analysis: {question_analysis}\n",
    "    \"\"\"\n",
    "    response = chat_session.send_message(prompt.format(user_input=user_input, question_analysis=question_analysis))\n",
    "    return response.text\n",
    "\n",
    "def learning_reinforcement_agent(user_input, question_analysis):\n",
    "    debug_print(\"Entering learning_reinforcement_agent\")\n",
    "    prompt = \"\"\"\n",
    "    # High-level instructions:\n",
    "    1. Summarize the key concepts and skills practiced in the recent problem-solving session.\n",
    "    2. Highlight connections between the math concepts and real-world applications, especially relating to the user's interests if available.\n",
    "    3. Suggest related problems or topics for further practice.\n",
    "    4. Provide encouragement and motivation for continued learning.\n",
    "\n",
    "    # Example input/output:\n",
    "    Input: \n",
    "    User: \"Can you summarize what we learned from solving 2x + 5 = 13?\"\n",
    "    Question Analysis: \"\n",
    "    Problem: Solve the equation: 2x + 5 = 13\n",
    "    Key Concepts: Linear equations, Algebraic manipulation\n",
    "    \"\n",
    "    Interest Profile: Sports (Basketball), Gaming\n",
    "\n",
    "    Output: \"Great question! Let's recap what we've learned:\n",
    "\n",
    "    1. Linear Equations: We worked with a basic linear equation in the form ax + b = c.\n",
    "    2. Algebraic Manipulation: We practiced isolating the variable (x) by using inverse operations.\n",
    "    3. Problem-Solving Strategy: We followed a step-by-step approach to solve the equation.\n",
    "\n",
    "    Real-world connections:\n",
    "    - In basketball, you might use similar equations to calculate scoring averages or predict future performance based on current stats.\n",
    "    - In gaming, developers use linear equations to create balanced game economies or determine character progression rates.\n",
    "\n",
    "    For further practice, try solving these related problems:\n",
    "    1. 3x - 7 = 14\n",
    "    2. -2x + 10 = 4\n",
    "    3. 0.5x + 3 = 8\n",
    "\n",
    "    Remember, each equation you solve strengthens your math skills, just like each practice session improves your game. Keep up the great work!\"\n",
    "\n",
    "    # Now, provide a learning summary based on the following input:\n",
    "    User Input: {user_input}\n",
    "    Question Analysis: {question_analysis}\n",
    "    \"\"\"\n",
    "    response = chat_session.send_message(prompt.format(user_input=user_input, question_analysis=question_analysis))\n",
    "    return response.text\n",
    "\n",
    "def chatbot(user_input):\n",
    "    debug_print(\"Entering chatbot\")\n",
    "    cleaned_input = preprocess_input(user_input)\n",
    "    category = classify_input(cleaned_input)\n",
    "    \n",
    "    if category == 'interest_profiling':\n",
    "        response = interest_profiling_agent(cleaned_input)\n",
    "        chat_session.history.append({\"role\": \"assistant\", \"parts\": [response]})\n",
    "    elif category == 'question_analysis':\n",
    "        question_analysis = question_analysis_agent(cleaned_input)\n",
    "        response = socratic_dialogue_agent(cleaned_input, question_analysis)\n",
    "        chat_session.history.append({\"role\": \"assistant\", \"parts\": [response]})\n",
    "    elif category == 'socratic_dialogue':\n",
    "        last_question_analysis = next((msg for msg in reversed(chat_session.history) if \"Question Analysis:\" in msg[\"parts\"][0]), None)\n",
    "        if last_question_analysis:\n",
    "            question_analysis = last_question_analysis[\"parts\"][0].split(\"Question Analysis:\", 1)[1].strip()\n",
    "            response = socratic_dialogue_agent(cleaned_input, question_analysis)\n",
    "        else:\n",
    "            response = \"I'm sorry, but I don't have the context of the previous question. Could you please restate the math problem you're working on?\"\n",
    "        chat_session.history.append({\"role\": \"assistant\", \"parts\": [response]})\n",
    "    elif category == 'answer_verification':\n",
    "        last_question_analysis = next((msg for msg in reversed(chat_session.history) if \"Question Analysis:\" in msg[\"parts\"][0]), None)\n",
    "        if last_question_analysis:\n",
    "            question_analysis = last_question_analysis[\"parts\"][0].split(\"Question Analysis:\", 1)[1].strip()\n",
    "            response = answer_verification_agent(cleaned_input, question_analysis)\n",
    "        else:\n",
    "            response = \"I'm sorry, but I don't have the context of the previous question. Could you please restate the math problem and your proposed answer?\"\n",
    "        chat_session.history.append({\"role\": \"assistant\", \"parts\": [response]})\n",
    "    elif category == 'learning_reinforcement':\n",
    "        last_question_analysis = next((msg for msg in reversed(chat_session.history) if \"Question Analysis:\" in msg[\"parts\"][0]), None)\n",
    "        if last_question_analysis:\n",
    "            question_analysis = last_question_analysis[\"parts\"][0].split(\"Question Analysis:\", 1)[1].strip()\n",
    "            response = learning_reinforcement_agent(cleaned_input, question_analysis)\n",
    "        else:\n",
    "            response = \"I'd be happy to summarize what we've learned, but I don't have the context of our previous discussion. Could you please remind me which math concept or problem we were working on?\"\n",
    "        chat_session.history.append({\"role\": \"assistant\", \"parts\": [response]})\n",
    "    else:\n",
    "        response = \"I'm not sure how to respond to that. Could you please ask about a specific math problem or concept you'd like help with?\"\n",
    "    \n",
    "    debug_print(f\"Chatbot response: {response}\")\n",
    "    return response\n",
    "\n",
    "# Test the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Welcome to the SAT Math Preparation Chatbot!\")\n",
    "    print(\"Let's start by understanding your interests.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(\"Thank you for using the SAT Math Preparation Chatbot. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        response = chatbot(user_input)\n",
    "        print(f\"Chatbot: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Used\n",
    "https://ai.google.dev/gemini-api/docs/function-calling/tutorial?lang=python\n",
    "\n",
    "    Home\n",
    "    Gemini API\n",
    "    Docs\n",
    "\n",
    "Was this helpful?\n",
    "Function calling tutorial\n",
    "\n",
    "Function calling makes it easier for you to get structured data outputs from generative models. You can then use these outputs to call other APIs and return the relevant response data to the model. In other words, function calling helps you connect generative models to external systems so that the generated content includes the most up-to-date and accurate information.\n",
    "\n",
    "You can provide Gemini models with descriptions of functions. These are functions that you write in the language of your app (that is, they're not Google Cloud Functions). The model may ask you to call a function and send back the result to help the model handle your query.\n",
    "\n",
    "If you haven't already, check out the Introduction to function calling to learn more. You can also try out this feature in Google Colab or view the example code in the Gemini API Cookbook repository.\n",
    "Example API for lighting control\n",
    "\n",
    "Imagine you have a basic lighting control system with an application programming interface (API) and you want to allow users to control the lights through simple text requests. You can use the Function Calling feature to interpret lighting change requests from users and translate them into API calls to set the lighting values. This hypothetical lighting control system lets you control the brightness of the light and it's color temperature, defined as two separate parameters:\n",
    "Parameter \tType \tRequired \tDescription\n",
    "brightness \tnumber \tyes \tLight level from 0 to 100. Zero is off and 100 is full brightness.\n",
    "colorTemperature \tstring \tyes \tColor temperature of the light fixture which can be daylight, cool or warm.\n",
    "\n",
    "For simplicity, this imaginary lighting system only has one light, so the user does not have to specify a room or location. Here is an example JSON request you could send to the lighting control API to change the light level to 50% using the daylight color temperature:\n",
    "\n",
    "{\n",
    "  \"brightness\": \"50\",\n",
    "  \"colorTemperature\": \"daylight\"\n",
    "}\n",
    "\n",
    "This tutorial shows you how to set up a Function Call for the Gemini API to interpret users lighting requests and map them to API settings to control a light's brightness and color temperature values.\n",
    "Before you begin: Set up your project and API key\n",
    "\n",
    "Before calling the Gemini API, you need to set up your project and configure your API key.\n",
    "\n",
    "Expand to view how to set up your project and API key\n",
    "\n",
    "Define an API function\n",
    "\n",
    "Create a function that makes an API request. This function should be defined within the code of your application, but could call services or APIs outside of your application. The Gemini API does not call this function directly, so you can control how and when this function is executed through your application code. For demonstration purposes, this tutorial defines a mock API function that just returns the requested lighting values:\n",
    "\n",
    "def set_light_values(brightness, color_temp):\n",
    "    \"\"\"Set the brightness and color temperature of a room light. (mock API).\n",
    "\n",
    "    Args:\n",
    "        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n",
    "        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the set brightness and color temperature.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"brightness\": brightness,\n",
    "        \"colorTemperature\": color_temp\n",
    "    }\n",
    "\n",
    "When you create a function to be used in a function call by the model, you should include as much detail as possible in the function and parameter descriptions. The generative model uses this information to determine which function to select and how to provide values for the parameters in the function call.\n",
    "Caution: For any production application, you should validate the data being passed to the API function from the model before executing the function.\n",
    "Note: For programing languages other than Python, you must create create a separate function declaration for your API. See the other language programming tutorials more details.\n",
    "Declare functions during model initialization\n",
    "\n",
    "When you want to use function calling with a model, you must declare your functions when you initialize the model object. You declare functions by setting the model's tools parameter:\n",
    "\n",
    "model = genai.GenerativeModel(model_name='gemini-1.5-flash',\n",
    "                              tools=[set_light_values])\n",
    "\n",
    "Generate a function call\n",
    "\n",
    "Once you have initialized model with your function declarations, you can prompt the model with the defined function. You should use use function calling using chat prompting (sendMessage()), since function calling generally benefits from having the context of previous prompts and responses.\n",
    "\n",
    "chat = model.start_chat()\n",
    "response = chat.send_message('Dim the lights so the room feels cozy and warm.')\n",
    "response.text\n",
    "\n",
    "The Python SDK's ChatSession object simplifies managing chat sessions by handling the conversation history for you. You can use the enable_automatic_function_calling to have the SDK automatically\n",
    "\n",
    "# Create a chat session that automatically makes suggested function calls\n",
    "chat = model.start_chat(enable_automatic_function_calling=True)\n",
    "\n",
    "Warning: Do not use this feature in production applications as there are no data input verification checks for automatic function calls.\n",
    "Parallel function calling\n",
    "\n",
    "In addition to basic function calling described above, you can also call multiple functions in a single turn. This section shows an example for how you can use parallel function calling.\n",
    "\n",
    "Define the tools.\n",
    "\n",
    "def power_disco_ball(power: bool) -> bool:\n",
    "    \"\"\"Powers the spinning disco ball.\"\"\"\n",
    "    print(f\"Disco ball is {'spinning!' if power else 'stopped.'}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def start_music(energetic: bool, loud: bool, bpm: int) -> str:\n",
    "    \"\"\"Play some music matching the specified parameters.\n",
    "\n",
    "    Args:\n",
    "      energetic: Whether the music is energetic or not.\n",
    "      loud: Whether the music is loud or not.\n",
    "      bpm: The beats per minute of the music.\n",
    "\n",
    "    Returns: The name of the song being played.\n",
    "    \"\"\"\n",
    "    print(f\"Starting music! {energetic=} {loud=}, {bpm=}\")\n",
    "    return \"Never gonna give you up.\"\n",
    "\n",
    "\n",
    "def dim_lights(brightness: float) -> bool:\n",
    "    \"\"\"Dim the lights.\n",
    "\n",
    "    Args:\n",
    "      brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n",
    "    \"\"\"\n",
    "    print(f\"Lights are now set to {brightness:.0%}\")\n",
    "    return True\n",
    "\n",
    "Now call the model with an instruction that could use all of the specified tools.\n",
    "\n",
    "# Set the model up with tools.\n",
    "house_fns = [power_disco_ball, start_music, dim_lights]\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", tools=house_fns)\n",
    "\n",
    "# Call the API.\n",
    "chat = model.start_chat()\n",
    "response = chat.send_message(\"Turn this place into a party!\")\n",
    "\n",
    "# Print out each of the function calls requested from this single call.\n",
    "for part in response.parts:\n",
    "    if fn := part.function_call:\n",
    "        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n",
    "        print(f\"{fn.name}({args})\")\n",
    "\n",
    "power_disco_ball(power=True)\n",
    "start_music(energetic=True, loud=True, bpm=120.0)\n",
    "dim_lights(brightness=0.3)\n",
    "\n",
    "Each of the printed results reflects a single function call that the model has requested. To send the results back, include the responses in the same order as they were requested.\n",
    "\n",
    "# Simulate the responses from the specified tools.\n",
    "responses = {\n",
    "    \"power_disco_ball\": True,\n",
    "    \"start_music\": \"Never gonna give you up.\",\n",
    "    \"dim_lights\": True,\n",
    "}\n",
    "\n",
    "# Build the response parts.\n",
    "response_parts = [\n",
    "    genai.protos.Part(function_response=genai.protos.FunctionResponse(name=fn, response={\"result\": val}))\n",
    "    for fn, val in responses.items()\n",
    "]\n",
    "\n",
    "response = chat.send_message(response_parts)\n",
    "print(response.text)\n",
    "\n",
    "Let's get this party started! I've turned on the disco ball, started playing some upbeat music, and dimmed the lights. ðŸŽ¶âœ¨  Get ready to dance! ðŸ•ºðŸ’ƒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Execution\n",
    "\n",
    "Enable code execution on the model\n",
    "\n",
    "You can enable code execution on the model, as shown here:\n",
    "\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ['API_KEY'])\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name='gemini-1.5-pro',\n",
    "    tools='code_execution')\n",
    "\n",
    "response = model.generate_content((\n",
    "    'What is the sum of the first 50 prime numbers? '\n",
    "    'Generate and run code for the calculation, and make sure you get all 50.'))\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "The output might look something like this:\n",
    "\n",
    "```python\n",
    "def is_prime(n):\n",
    "  \"\"\"Checks if a number is prime.\"\"\"\n",
    "  if n <= 1:\n",
    "    return False\n",
    "  for i in range(2, int(n**0.5) + 1):\n",
    "    if n % i == 0:\n",
    "      return False\n",
    "  return True\n",
    "\n",
    "def sum_of_primes(n):\n",
    "  \"\"\"Calculates the sum of the first n prime numbers.\"\"\"\n",
    "  primes = []\n",
    "  i = 2\n",
    "  while len(primes) < n:\n",
    "    if is_prime(i):\n",
    "      primes.append(i)\n",
    "    i += 1\n",
    "  return sum(primes)\n",
    "\n",
    "# Calculate the sum of the first 50 prime numbers\n",
    "sum_of_first_50_primes = sum_of_primes(50)\n",
    "\n",
    "print(f\"The sum of the first 50 prime numbers is: {sum_of_first_50_primes}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "1. **`is_prime(n)` Function:**\n",
    "   - Takes an integer `n` as input.\n",
    "   - Returns `False` for numbers less than or equal to 1 (not prime).\n",
    "   - Iterates from 2 up to the square root of `n`. If `n` is divisible by any\n",
    "     number in this range, it's not prime, and we return `False`.\n",
    "   - If the loop completes without finding a divisor, the number is prime, and\n",
    "     we return `True`.\n",
    "\n",
    "2. **`sum_of_primes(n)` Function:**\n",
    "   - Takes an integer `n` (number of primes desired) as input.\n",
    "   - Initializes an empty list `primes` to store the prime numbers.\n",
    "   - Starts a loop, iterating through numbers starting from 2.\n",
    "   - For each number `i`, it checks if it's prime using the `is_prime()` function.\n",
    "   - If `i` is prime, it's appended to the `primes` list.\n",
    "   - The loop continues until the `primes` list has `n` prime numbers.\n",
    "   - Finally, it calculates and returns the sum of all the prime numbers in the\n",
    "     `primes` list.\n",
    "\n",
    "3. **Main Part:**\n",
    "   - Calls `sum_of_primes(50)` to get the sum of the first 50 prime numbers.\n",
    "   - Prints the result.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "The sum of the first 50 prime numbers is: 5117\n",
    "```\n",
    "\n",
    "Enable code execution on the request\n",
    "\n",
    "Alternatively, you can enable code execution on the call to generate_content:\n",
    "\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ['API_KEY'])\n",
    "\n",
    "model = genai.GenerativeModel(model_name='gemini-1.5-pro')\n",
    "\n",
    "response = model.generate_content(\n",
    "    ('What is the sum of the first 50 prime numbers? '\n",
    "    'Generate and run code for the calculation, and make sure you get all 50.'),\n",
    "    tools='code_execution')\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "Use code execution in chat\n",
    "\n",
    "You can also use code execution as part of a chat.\n",
    "\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ['API_KEY'])\n",
    "\n",
    "model = genai.GenerativeModel(model_name='gemini-1.5-pro',\n",
    "                              tools='code_execution')\n",
    "\n",
    "chat = model.start_chat()\n",
    "\n",
    "response = chat.send_message((\n",
    "    'What is the sum of the first 50 prime numbers? '\n",
    "    'Generate and run code for the calculation, and make sure you get all 50.'))\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "Code execution versus function calling\n",
    "\n",
    "Code execution and function calling are similar features:\n",
    "\n",
    "    Code execution lets the model run code in the API backend in a fixed, isolated environment.\n",
    "    Function calling lets you run the functions that the model requests, in whatever environment you want.\n",
    "\n",
    "In general you should prefer to use code execution if it can handle your use case. Code execution is simpler to use (you just enable it) and resolves in a single GenerateContent request (thus incurring a single charge). Function calling takes an additional GenerateContent request to send back the output from each function call (thus incurring multiple charges).\n",
    "\n",
    "For most cases, you should use function calling if you have your own functions that you want to run locally, and you should use code execution if you'd like the API to write and run Python code for you and return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hellloo = model.generate_content(\"1+1\")\n",
    "\n",
    "print(hellloo.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can write code to calculate that. First, I need a function to check if a number is prime. Then I can iterate through numbers, check if they're prime, and add them until I have 50 primes. \n",
      "\n",
      "\n",
      "``` python\n",
      "def is_prime(n):\n",
      "    \"\"\"Checks if n is a prime number (greater than 1)\"\"\"\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    for i in range(2, int(n**0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "\n",
      "primes = []\n",
      "num = 2\n",
      "while len(primes) < 50:\n",
      "    if is_prime(num):\n",
      "        primes.append(num)\n",
      "    num += 1\n",
      "\n",
      "print(sum(primes))\n",
      "\n",
      "```\n",
      "```\n",
      "5117\n",
      "\n",
      "```\n",
      "The sum of the first 50 prime numbers is 5117. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyDgJ_0xf87nMi-OYpIqo9BA9EZrsYCrlGo\")\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n",
    "code_model = genai.GenerativeModel(model_name='gemini-1.5-pro', tools='code_execution')\n",
    "\n",
    "response = code_model.generate_content(\n",
    "    ('What is the sum of the first 50 prime numbers? '\n",
    "    'Generate and run code for the calculation, and make sure you get all 50.'),\n",
    "    tools='code_execution')\n",
    "\n",
    "print(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_disco_ball(power: bool) -> bool:\n",
    "    \"\"\"Powers the spinning disco ball.\"\"\"\n",
    "    print(f\"Disco ball is {'spinning!' if power else 'stopped.'}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def start_music(energetic: bool, loud: bool, bpm: int) -> str:\n",
    "    \"\"\"Play some music matching the specified parameters.\n",
    "\n",
    "    Args:\n",
    "      energetic: Whether the music is energetic or not.\n",
    "      loud: Whether the music is loud or not.\n",
    "      bpm: The beats per minute of the music.\n",
    "\n",
    "    Returns: The name of the song being played.\n",
    "    \"\"\"\n",
    "    print(f\"Starting music! {energetic=} {loud=}, {bpm=}\")\n",
    "    return \"Never gonna give you up.\"\n",
    "\n",
    "\n",
    "def dim_lights(brightness: float) -> bool:\n",
    "    \"\"\"Dim the lights.\n",
    "\n",
    "    Args:\n",
    "      brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n",
    "    \"\"\"\n",
    "    print(f\"Lights are now set to {brightness:.0%}\")\n",
    "    return True\n",
    "\n",
    "# Set the model up with tools.\n",
    "house_fns = [power_disco_ball, start_music, dim_lights]\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", tools=house_fns)\n",
    "\n",
    "# Call the API.\n",
    "chat = model.start_chat()\n",
    "response = chat.send_message(\"Turn this place into a party!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "power_disco_ball(power=True)\n",
      "start_music(loud=True, energetic=True, bpm=120.0)\n",
      "dim_lights(brightness=0.5)\n"
     ]
    }
   ],
   "source": [
    "# Print out each of the function calls requested from this single call.\n",
    "for part in response.parts:\n",
    "    if fn := part.function_call:\n",
    "        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n",
    "        print(f\"{fn.name}({args})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "whichOneof",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m response \u001b[38;5;241m=\u001b[39m chat\u001b[38;5;241m.\u001b[39msend_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm feeling sleepy, let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms wind down.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\63916\\Downloads\\test-pk\\venv\\Lib\\site-packages\\google\\generativeai\\types\\generation_types.py:465\u001b[0m, in \u001b[0;36mBaseGenerateContentResponse.text\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    462\u001b[0m         texts\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutcome_result\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, part\u001b[38;5;241m.\u001b[39mcode_execution_result\u001b[38;5;241m.\u001b[39moutput, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m     part_type \u001b[38;5;241m=\u001b[39m \u001b[43mprotos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhichOneof\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert `part.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpart_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` to text.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(texts)\n",
      "\u001b[1;31mAttributeError\u001b[0m: whichOneof"
     ]
    }
   ],
   "source": [
    "# Simulate the responses from the specified tools.\n",
    "responses = {\n",
    "    \"power_disco_ball\": True,\n",
    "    \"start_music\": \"Never gonna give you up.\",\n",
    "    \"dim_lights\": True,\n",
    "}\n",
    "\n",
    "# Build the response parts.\n",
    "response_parts = [\n",
    "    genai.protos.Part(function_response=genai.protos.FunctionResponse(name=fn, response={\"result\": val}))\n",
    "    for fn, val in responses.items()\n",
    "]\n",
    "\n",
    "response = chat.send_message(response_parts)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
